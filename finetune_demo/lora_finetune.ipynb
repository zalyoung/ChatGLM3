{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {
    "collapsed": false,
    "id": "89b89f64d8f8053d",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 单卡GPU 进行 ChatGLM3-6B模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 硬件需求\n",
    "显存：24GB及以上（推荐使用30系或A10等sm80架构以上的NVIDIA显卡进行尝试）\n",
    "内存：16GB\n",
    "RAM: 2.9 /16 GB\n",
    "GPU RAM: 15.5/16.0 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {
    "collapsed": false,
    "id": "a7bd9a514ed09ea6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 0. 环境检查\n",
    "首先，先检查代码的运行地址，确保运行地址处于 `finetune_demo` 中。\n",
    "并且，确保已经安装了 `requirements.txt`中的依赖。\n",
    "\n",
    "> 本 demo 中，不需要使用 deepspeed, mpi4py 两个依赖，如果您安装这两个依赖遇到问题，可以不安装这两个依赖。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7703109d1443346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T05:29:22.200365Z",
     "start_time": "2024-04-14T05:29:22.080929Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/ChatGLM3/finetune_demo\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already satisfied: jieba>=0.42.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.42.1)\n",
      "Requirement already satisfied: ruamel_yaml>=0.18.6 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (0.18.6)\n",
      "Requirement already satisfied: rouge_chinese>=1.0.3 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.0.3)\n",
      "Requirement already satisfied: jupyter>=1.0.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (1.0.0)\n",
      "Requirement already satisfied: datasets>=2.18.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (2.19.1)\n",
      "Requirement already satisfied: peft>=0.10.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (0.11.1)\n",
      "Requirement already satisfied: deepspeed==0.13.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (0.13.1)\n",
      "Requirement already satisfied: mpi4py>=3.1.5 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (3.1.6)\n",
      "Requirement already satisfied: hjson in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: ninja in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (1.11.1.1)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (23.2)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (9.0.0)\n",
      "Requirement already satisfied: pydantic in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (2.7.1)\n",
      "Requirement already satisfied: pynvml in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (11.5.0)\n",
      "Requirement already satisfied: torch in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (2.3.0)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from deepspeed==0.13.1->-r requirements.txt (line 7)) (4.66.4)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ruamel_yaml>=0.18.6->-r requirements.txt (line 2)) (0.2.8)\n",
      "Requirement already satisfied: six in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from rouge_chinese>=1.0.3->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: notebook in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 4)) (7.1.3)\n",
      "Requirement already satisfied: qtconsole in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 4)) (5.5.2)\n",
      "Requirement already satisfied: jupyter-console in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 4)) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 4)) (7.16.4)\n",
      "Requirement already satisfied: ipykernel in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 4)) (6.29.3)\n",
      "Requirement already satisfied: ipywidgets in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 4)) (8.1.2)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (3.14.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.18.0->-r requirements.txt (line 5)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (0.23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from datasets>=2.18.0->-r requirements.txt (line 5)) (6.0.1)\n",
      "Requirement already satisfied: transformers in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from peft>=0.10.0->-r requirements.txt (line 6)) (4.37.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from peft>=0.10.0->-r requirements.txt (line 6)) (0.26.1)\n",
      "Requirement already satisfied: safetensors in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from peft>=0.10.0->-r requirements.txt (line 6)) (0.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 5)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 5)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 5)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from aiohttp->datasets>=2.18.0->-r requirements.txt (line 5)) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets>=2.18.0->-r requirements.txt (line 5)) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.18.0->-r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.18.0->-r requirements.txt (line 5)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.18.0->-r requirements.txt (line 5)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=2.18.0->-r requirements.txt (line 5)) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (12.4.127)\n",
      "Requirement already satisfied: comm>=0.1.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (8.24.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (8.6.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.6.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (6.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 4)) (3.0.10)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter-console->jupyter>=1.0.0->-r requirements.txt (line 4)) (3.0.42)\n",
      "Requirement already satisfied: pygments in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter-console->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.18.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.14.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.27.1)\n",
      "Requirement already satisfied: jupyterlab<4.2,>=4.1.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.1.8)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from pandas->datasets>=2.18.0->-r requirements.txt (line 5)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from pandas->datasets>=2.18.0->-r requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from pandas->datasets>=2.18.0->-r requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from pydantic->deepspeed==0.13.1->-r requirements.txt (line 7)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from pydantic->deepspeed==0.13.1->-r requirements.txt (line 7)) (2.18.2)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from qtconsole->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from transformers->peft>=0.10.0->-r requirements.txt (line 6)) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from transformers->peft>=0.10.0->-r requirements.txt (line 6)) (0.15.2)\n",
      "Requirement already satisfied: webencodings in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.5.1)\n",
      "Requirement already satisfied: decorator in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.2.2)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.3.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.5.3)\n",
      "Requirement already satisfied: overrides>=5.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.20.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.27.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyterlab<4.2,>=4.1.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.2.5)\n",
      "Requirement already satisfied: babel>=2.10 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.14.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.9.25)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (4.22.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nbformat>=5.7->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.19.1)\n",
      "Requirement already satisfied: wcwidth in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from sympy->torch->deepspeed==0.13.1->-r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (21.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.8.4)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.18.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.1.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r requirements.txt (line 4)) (0.2.2)\n",
      "Requirement already satisfied: fqdn in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.4)\n",
      "Requirement already satisfied: uri-template in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter>=1.0.0->-r requirements.txt (line 4)) (2.9.0.20240316)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already satisfied: nltk in /root/miniconda3/envs/peft/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/envs/peft/lib/python3.11/site-packages (from nltk) (4.66.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!pip install -r requirements.txt\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50e92810011977",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /media/zr/Data/Code/ChatGLM3/finetune_demo/data/AdvertiseGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T05:29:23.809255Z",
     "start_time": "2024-04-14T05:29:22.202731Z"
    },
    "cellView": "form",
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {
    "collapsed": false,
    "id": "a1b7a99923349056",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c87410a24d844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:23:41.282431Z",
     "start_time": "2024-04-14T05:29:23.810692Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17c87410a24d844f",
    "outputId": "e347fc7d-875e-40c9-c682-3e064100476b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:02<00:00,  2.35it/s]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.0312\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "Map (num_proc=8): 100%|███████| 114599/114599 [00:04<00:00, 24920.01 examples/s]\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "Map (num_proc=8): 100%|████████████| 1070/1070 [00:00<00:00, 2008.14 examples/s]\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "Map (num_proc=8): 100%|████████████| 1070/1070 [00:00<00:00, 1918.21 examples/s]\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 600\n",
      "  Number of trainable parameters = 1,949,696\n",
      "{'loss': 4.5438, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.0}         \n",
      "{'loss': 5.068, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.6682, 'learning_rate': 4.75e-05, 'epoch': 0.0}                       \n",
      "{'loss': 4.4846, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.0}          \n",
      "{'loss': 4.3582, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.1549, 'learning_rate': 4.5e-05, 'epoch': 0.0}                        \n",
      "{'loss': 3.9531, 'learning_rate': 4.4166666666666665e-05, 'epoch': 0.0}         \n",
      "{'loss': 4.1459, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8131, 'learning_rate': 4.25e-05, 'epoch': 0.0}                       \n",
      "{'loss': 4.1576, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.0}          \n",
      " 17%|██████▊                                  | 100/600 [00:22<01:38,  5.10it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:27<11:07, 13.90s/it]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:33<08:03, 10.28s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [01:00<12:52, 16.79s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [01:03<08:51, 11.81s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [01:31<12:34, 17.15s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [01:35<09:15, 12.92s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [01:36<06:33,  9.37s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [01:40<05:18,  7.77s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [01:42<03:56,  5.91s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [02:10<08:13, 12.65s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [02:38<10:57, 17.31s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [02:42<08:06, 13.14s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [02:43<05:40,  9.47s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [02:53<05:44,  9.85s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [02:56<04:25,  7.80s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [02:58<03:17,  5.97s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [03:00<02:27,  4.62s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [03:05<02:26,  4.73s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [03:07<01:56,  3.87s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [03:11<01:54,  3.95s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [03:39<05:14, 11.23s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [03:43<04:02,  8.98s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [03:46<03:11,  7.37s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [03:49<02:28,  5.95s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [03:52<02:02,  5.12s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [03:56<01:51,  4.85s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [04:24<04:19, 11.81s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [04:29<03:21,  9.58s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [04:33<02:42,  8.10s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [04:39<02:20,  7.40s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [04:42<01:48,  6.04s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [04:43<01:19,  4.67s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [05:12<03:07, 11.71s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [05:15<02:17,  9.17s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [05:17<01:36,  6.93s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [05:18<01:09,  5.36s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [05:21<00:56,  4.70s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [05:23<00:40,  3.69s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [05:25<00:33,  3.34s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [05:27<00:25,  2.87s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [05:29<00:20,  2.60s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [05:31<00:16,  2.37s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [05:33<00:13,  2.19s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [06:01<00:49,  9.99s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [06:02<00:29,  7.35s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [06:04<00:17,  5.70s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [06:06<00:09,  4.64s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [06:34<00:11, 11.69s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 50/50 [06:38<00:00,  9.26s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.593 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 27.111525999999998, 'eval_rouge-2': 4.722338, 'eval_rouge-l': 19.023504000000003, 'eval_bleu-4': 0.018926051245359097, 'eval_runtime': 402.6168, 'eval_samples_per_second': 0.124, 'eval_steps_per_second': 0.124, 'epoch': 0.0}\n",
      " 17%|██████▊                                  | 100/600 [07:04<01:38,  5.10it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [06:39<00:00,  9.26s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-100\n",
      "/root/miniconda3/envs/peft/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/103caa40027ebfd8450289ca2f278eac4ff26405/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 4.2555, 'learning_rate': 4.0833333333333334e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8492, 'learning_rate': 4e-05, 'epoch': 0.0}                          \n",
      "{'loss': 3.7717, 'learning_rate': 3.9166666666666665e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.618, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.7145, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.5637, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.4539, 'learning_rate': 3.5833333333333335e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7648, 'learning_rate': 3.5e-05, 'epoch': 0.0}                        \n",
      "{'loss': 3.6949, 'learning_rate': 3.4166666666666666e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8676, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.0}         \n",
      " 33%|█████████████▋                           | 200/600 [07:27<01:41,  3.95it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:28<11:18, 14.13s/it]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:30<07:16,  9.30s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:34<05:35,  7.29s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:37<04:20,  5.78s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:39<03:22,  4.60s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:43<02:58,  4.15s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:46<02:38,  3.77s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:49<02:25,  3.56s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:50<01:53,  2.85s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:53<01:52,  2.89s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:55<01:44,  2.76s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:58<01:41,  2.73s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [01:00<01:35,  2.66s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [01:05<01:48,  3.11s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [01:07<01:38,  2.89s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [01:10<01:36,  2.91s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [01:13<01:29,  2.81s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [01:15<01:28,  2.85s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [01:17<01:12,  2.43s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [01:20<01:12,  2.51s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [01:23<01:21,  2.92s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [01:28<01:30,  3.37s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [01:31<01:23,  3.21s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [01:34<01:18,  3.14s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [01:38<01:20,  3.34s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [01:44<01:35,  4.13s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [01:47<01:24,  3.84s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [01:50<01:14,  3.55s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [01:54<01:15,  3.77s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [01:57<01:09,  3.66s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [02:00<01:01,  3.43s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [02:03<00:55,  3.24s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [02:07<00:57,  3.58s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [02:11<00:53,  3.57s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [02:14<00:49,  3.51s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [02:17<00:41,  3.22s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [02:20<00:38,  3.18s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [02:23<00:36,  3.28s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [02:29<00:40,  4.01s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [02:32<00:33,  3.74s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [02:35<00:26,  3.33s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [02:37<00:21,  3.10s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [02:39<00:16,  2.77s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [02:42<00:14,  2.81s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [02:45<00:11,  2.80s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [02:47<00:08,  2.73s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [02:51<00:06,  3.07s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [02:56<00:03,  3.54s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 28.878322000000008, 'eval_rouge-2': 5.461676, 'eval_rouge-l': 23.014594, 'eval_bleu-4': 0.02683963956630329, 'eval_runtime': 182.2637, 'eval_samples_per_second': 0.274, 'eval_steps_per_second': 0.274, 'epoch': 0.0}\n",
      " 33%|█████████████▋                           | 200/600 [10:29<01:41,  3.95it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [02:58<00:00,  3.19s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-200\n",
      "/root/miniconda3/envs/peft/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/103caa40027ebfd8450289ca2f278eac4ff26405/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.4729, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7857, 'learning_rate': 3.1666666666666666e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8598, 'learning_rate': 3.0833333333333335e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.699, 'learning_rate': 3e-05, 'epoch': 0.0}                           \n",
      "{'loss': 3.6088, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.0}          \n",
      "{'loss': 3.8934, 'learning_rate': 2.8333333333333335e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6576, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7547, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.8318, 'learning_rate': 2.5833333333333336e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6459, 'learning_rate': 2.5e-05, 'epoch': 0.0}                        \n",
      " 50%|████████████████████▌                    | 300/600 [10:52<01:00,  4.94it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:03<01:20,  1.67s/it]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:05<01:36,  2.05s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:09<01:52,  2.44s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:12<02:04,  2.78s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:15<02:05,  2.86s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:17<01:56,  2.70s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:20<01:50,  2.63s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:23<02:01,  2.96s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:26<01:47,  2.69s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:28<01:46,  2.73s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:31<01:39,  2.61s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:33<01:33,  2.52s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:36<01:34,  2.63s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:39<01:39,  2.84s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:42<01:34,  2.78s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:45<01:32,  2.80s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:47<01:28,  2.78s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:51<01:30,  2.92s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:52<01:17,  2.58s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [01:21<04:57, 10.26s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [01:49<07:18, 15.65s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [01:53<05:28, 12.16s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [01:57<04:10,  9.65s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [02:00<03:11,  7.64s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [02:03<02:34,  6.45s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [02:08<02:18,  6.04s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [02:11<01:49,  5.00s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [02:14<01:29,  4.28s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [02:18<01:23,  4.19s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [02:21<01:14,  3.94s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [02:24<01:04,  3.57s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [02:27<00:59,  3.52s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [02:31<00:59,  3.71s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [02:34<00:50,  3.39s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [02:37<00:46,  3.32s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [03:05<02:19, 10.75s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [03:08<01:42,  8.53s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [03:11<01:15,  6.85s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [03:13<00:54,  5.42s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [03:17<00:44,  4.90s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [03:20<00:34,  4.28s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [03:24<00:28,  4.11s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [03:25<00:20,  3.41s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [03:29<00:16,  3.39s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [03:32<00:13,  3.42s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [03:35<00:09,  3.26s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [03:38<00:06,  3.28s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [03:41<00:03,  3.11s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 28.80281, 'eval_rouge-2': 5.695158, 'eval_rouge-l': 22.73867, 'eval_bleu-4': 0.027112481577817652, 'eval_runtime': 227.9349, 'eval_samples_per_second': 0.219, 'eval_steps_per_second': 0.219, 'epoch': 0.0}\n",
      " 50%|████████████████████▌                    | 300/600 [14:40<01:00,  4.94it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [03:44<00:00,  3.08s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-300\n",
      "/root/miniconda3/envs/peft/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/103caa40027ebfd8450289ca2f278eac4ff26405/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.5762, 'learning_rate': 2.4166666666666667e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.7396, 'learning_rate': 2.3333333333333336e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6547, 'learning_rate': 2.25e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.5348, 'learning_rate': 2.1666666666666667e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.5229, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.5582, 'learning_rate': 2e-05, 'epoch': 0.0}                          \n",
      "{'loss': 3.3359, 'learning_rate': 1.9166666666666667e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.4609, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.5131, 'learning_rate': 1.75e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.6998, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.0}         \n",
      " 67%|███████████████████████████▎             | 400/600 [15:04<00:44,  4.52it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:03<01:29,  1.86s/it]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:06<01:39,  2.12s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:34<09:02, 11.79s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:37<06:39,  8.88s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [01:05<11:11, 15.26s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [01:08<08:00, 11.18s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [01:10<05:49,  8.32s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [01:13<04:31,  6.63s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [01:41<08:48, 13.21s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [01:44<06:32, 10.07s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [02:12<09:50, 15.53s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [02:15<07:15, 11.78s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [02:18<05:29,  9.14s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [02:22<04:23,  7.52s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [02:24<03:25,  6.05s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [02:28<02:56,  5.34s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [02:31<02:27,  4.62s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [02:33<02:02,  3.94s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [02:36<01:50,  3.69s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [02:39<01:36,  3.33s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [02:42<01:30,  3.24s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [02:45<01:29,  3.31s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [02:48<01:20,  3.11s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [02:52<01:23,  3.33s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [02:55<01:19,  3.30s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [02:59<01:21,  3.56s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [03:02<01:10,  3.19s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [03:04<01:03,  3.03s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [03:08<01:03,  3.19s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [03:13<01:12,  3.81s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [03:16<01:05,  3.66s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [03:19<00:58,  3.47s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [03:22<00:50,  3.17s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [03:26<00:49,  3.29s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [03:30<00:52,  3.78s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [03:33<00:45,  3.50s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [03:36<00:40,  3.39s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [03:39<00:34,  3.17s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [03:43<00:32,  3.25s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [03:46<00:30,  3.38s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [03:50<00:27,  3.43s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [03:53<00:23,  3.40s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [03:56<00:20,  3.37s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [03:59<00:15,  3.18s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [04:02<00:12,  3.22s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [04:05<00:08,  2.91s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [04:07<00:05,  2.70s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [04:10<00:02,  2.86s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 29.545783999999994, 'eval_rouge-2': 5.270096, 'eval_rouge-l': 23.48314, 'eval_bleu-4': 0.025785937948162977, 'eval_runtime': 257.6403, 'eval_samples_per_second': 0.194, 'eval_steps_per_second': 0.194, 'epoch': 0.0}\n",
      " 67%|███████████████████████████▎             | 400/600 [19:21<00:44,  4.52it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [04:14<00:00,  3.20s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-400\n",
      "/root/miniconda3/envs/peft/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/103caa40027ebfd8450289ca2f278eac4ff26405/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.4346, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.2463, 'learning_rate': 1.5e-05, 'epoch': 0.0}                        \n",
      "{'loss': 3.6506, 'learning_rate': 1.4166666666666668e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6518, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6951, 'learning_rate': 1.25e-05, 'epoch': 0.0}                       \n",
      "{'loss': 3.1984, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.6609, 'learning_rate': 1.0833333333333334e-05, 'epoch': 0.0}         \n",
      "{'loss': 3.4213, 'learning_rate': 1e-05, 'epoch': 0.0}                          \n",
      "{'loss': 3.6156, 'learning_rate': 9.166666666666666e-06, 'epoch': 0.0}          \n",
      "{'loss': 3.6346, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.0}          \n",
      " 83%|██████████████████████████████████▏      | 500/600 [19:45<00:24,  4.16it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:03<01:30,  1.89s/it]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:07<01:56,  2.47s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:35<09:12, 12.02s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:38<06:44,  8.98s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:41<05:14,  7.15s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:44<04:05,  5.72s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:48<03:32,  5.05s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:51<03:06,  4.54s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:54<02:38,  3.95s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:57<02:23,  3.67s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [01:25<07:00, 11.07s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [01:28<05:24,  8.76s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [01:31<04:12,  7.00s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [01:35<03:29,  5.98s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [01:37<02:49,  4.97s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [01:41<02:27,  4.47s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [01:44<02:08,  4.01s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [01:46<01:48,  3.51s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [01:48<01:29,  2.97s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [01:54<01:54,  3.95s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [01:57<01:40,  3.60s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [02:00<01:34,  3.51s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [02:03<01:23,  3.23s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [02:06<01:23,  3.33s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [02:10<01:20,  3.34s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [02:16<01:37,  4.25s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [02:18<01:22,  3.76s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [02:22<01:19,  3.77s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [02:27<01:19,  3.99s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [02:31<01:15,  3.99s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [02:34<01:10,  3.90s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [02:37<01:00,  3.57s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [02:42<01:04,  4.01s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [02:46<00:56,  3.77s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [02:49<00:51,  3.67s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [02:52<00:43,  3.34s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [02:57<00:46,  3.87s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [02:59<00:38,  3.52s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [03:04<00:37,  3.72s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [03:06<00:30,  3.35s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [03:09<00:25,  3.22s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [03:13<00:24,  3.48s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [03:16<00:19,  3.25s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [03:19<00:16,  3.35s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [03:22<00:12,  3.13s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [03:24<00:08,  2.85s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [03:27<00:05,  2.72s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [03:29<00:02,  2.79s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 29.787561999999998, 'eval_rouge-2': 6.4766840000000006, 'eval_rouge-l': 24.189753999999997, 'eval_bleu-4': 0.03030962071666594, 'eval_runtime': 216.855, 'eval_samples_per_second': 0.231, 'eval_steps_per_second': 0.231, 'epoch': 0.0}\n",
      " 83%|██████████████████████████████████▏      | 500/600 [23:22<00:24,  4.16it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [03:33<00:00,  2.99s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-500\n",
      "/root/miniconda3/envs/peft/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/103caa40027ebfd8450289ca2f278eac4ff26405/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.5041, 'learning_rate': 7.5e-06, 'epoch': 0.0}                        \n",
      "{'loss': 3.5637, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.0}          \n",
      "{'loss': 3.4219, 'learning_rate': 5.833333333333334e-06, 'epoch': 0.0}          \n",
      "{'loss': 3.6002, 'learning_rate': 5e-06, 'epoch': 0.0}                          \n",
      "{'loss': 3.6074, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.0}          \n",
      "{'loss': 3.6646, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.0}         \n",
      "{'loss': 3.7049, 'learning_rate': 2.5e-06, 'epoch': 0.0}                        \n",
      "{'loss': 3.3902, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.01}        \n",
      "{'loss': 3.4977, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.01}         \n",
      "{'loss': 3.6926, 'learning_rate': 0.0, 'epoch': 0.01}                           \n",
      "100%|█████████████████████████████████████████| 600/600 [23:45<00:00,  4.09it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 1\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:03<01:27,  1.83s/it]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:31<10:00, 12.78s/it]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:59<14:09, 18.48s/it]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [01:03<09:54, 13.22s/it]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [01:06<07:18,  9.98s/it]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [01:09<05:28,  7.64s/it]\u001b[A\n",
      " 16%|███████                                     | 8/50 [01:13<04:29,  6.41s/it]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [01:16<03:45,  5.50s/it]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [01:19<03:04,  4.62s/it]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [01:22<02:41,  4.15s/it]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [01:50<07:13, 11.40s/it]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [01:53<05:32,  8.99s/it]\u001b[A\n",
      " 28%|████████████                               | 14/50 [01:56<04:20,  7.23s/it]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [02:00<03:30,  6.02s/it]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [02:03<02:53,  5.10s/it]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [02:06<02:29,  4.52s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [02:09<02:09,  4.05s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [02:11<01:49,  3.54s/it]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [02:13<01:30,  3.01s/it]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [02:18<01:44,  3.61s/it]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [02:23<01:57,  4.20s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [02:27<01:46,  3.95s/it]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [02:29<01:31,  3.54s/it]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [02:33<01:28,  3.55s/it]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [02:36<01:24,  3.52s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [02:42<01:34,  4.11s/it]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [02:45<01:23,  3.82s/it]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [02:48<01:17,  3.71s/it]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [02:51<01:09,  3.50s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [02:55<01:06,  3.50s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [02:58<01:03,  3.51s/it]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [03:01<00:56,  3.30s/it]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [03:06<01:01,  3.82s/it]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [03:10<00:56,  3.75s/it]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [03:14<00:53,  3.85s/it]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [03:18<00:49,  3.81s/it]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [03:23<00:51,  4.29s/it]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [03:26<00:41,  3.81s/it]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [03:30<00:38,  3.87s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [03:32<00:31,  3.45s/it]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [03:35<00:26,  3.29s/it]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [03:39<00:24,  3.45s/it]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [03:42<00:19,  3.22s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [03:45<00:16,  3.26s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [03:48<00:12,  3.13s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [03:50<00:08,  2.95s/it]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [03:53<00:05,  2.79s/it]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [03:56<00:02,  2.77s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 29.566950000000002, 'eval_rouge-2': 6.40703, 'eval_rouge-l': 24.198468000000002, 'eval_bleu-4': 0.031190352646288506, 'eval_runtime': 241.8133, 'eval_samples_per_second': 0.207, 'eval_steps_per_second': 0.207, 'epoch': 0.01}\n",
      "100%|█████████████████████████████████████████| 600/600 [27:47<00:00,  4.09it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [03:58<00:00,  2.65s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/tmp-checkpoint-600\n",
      "/root/miniconda3/envs/peft/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/103caa40027ebfd8450289ca2f278eac4ff26405/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1668.3549, 'train_samples_per_second': 0.36, 'train_steps_per_second': 0.36, 'train_loss': 3.7427180989583335, 'epoch': 0.01}\n",
      "100%|█████████████████████████████████████████| 600/600 [27:48<00:00,  2.78s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 1\n",
      "100%|████████████████████████████████████▉| 1068/1070 [1:21:58<00:06,  3.22s/it]"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 NCCL_P2P_DISABLE=\"1\" NCCL_IB_DISABLE=\"1\" python finetune_hf.py  data/AdvertiseGen_fix  THUDM/chatglm3-6b  configs/lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9418f6c5c264601",
   "metadata": {
    "collapsed": false,
    "id": "d9418f6c5c264601",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5060015c24e97ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:23:52.725227Z",
     "start_time": "2024-04-14T06:23:41.284552Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5060015c24e97ae",
    "outputId": "d3f03d0d-46bf-4c74-9b00-dc0160da0e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:02<00:00,  2.45it/s]\r\n",
      "Setting eos_token is not supported, use the default one.\r\n",
      "Setting pad_token is not supported, use the default one.\r\n",
      "Setting unk_token is not supported, use the default one.\r\n",
      "这款连衣裙采用压褶的版型设计，不规则的木耳边拼接，修饰了腰线，使得身材更加修长，不规则的压褶设计，增加了层次感，不规则的压褶，修饰了腰线，拉长腿部比例，显瘦又性感，套头的设计，方便穿脱，不规则的压褶，增加层次感，视觉上拉长腿部比例，百褶的网纱拼接，增加了层次感，整体气质优雅。\r\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 NCCL_P2P_DISABLE=\"1\" NCCL_IB_DISABLE=\"1\" python inference_hf.py output/checkpoint-600/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {
    "collapsed": false,
    "id": "18cd83087f096094",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
